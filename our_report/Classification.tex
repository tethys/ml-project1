\subsection{Logistic Regression}

We applied Logistic regression and Penalized logistic Regression using gradient descent. Best results are obtained on the train accuracy of $97.14\%$ with value 10.0 for $\alpha$. We obtained the optimal $\alpha$ value for logistic regression classifier using cross validation technique where we split $50\%$ of the data for validation.

In penalized logistic regression, the choice of penalty factor ($\lambda$) is crucial. Therefore, we used cross validation procedure to estimate $\lambda$ when we divide $50\%$ of the data as test data and $50\%$ of the data as training data. The value of $\lambda$ varies from $10^{-2}$ to $10^3$ with 400 points in between. As can be seen in Figure \ref{fig:Lambda_pLr}, for smaller $\lambda$ values, training error is much lower than the test error while the test error gets as high as the training error  as the $\lambda$ increases which is a sign of under-fitting. 

For smaller $\lambda$ values, the training and test error estimated in penalized logistic regression shows very similar behavior in the training and test error estimated in logistic regression. The same $\alpha$ value that is obtained for the logistic regression method also gives the best accuracy for penalized logistic regression method.

\begin{figure}[h]
  \begin{subfigure}[b]{0.5\textwidth}
   \includegraphics[clip, trim=4cm 9cm 3cm 10cm, width=\textwidth]{figures/Lambda_pLG.pdf}
    \caption{Train and test error assessment with penalty factor ($\lambda$) for penalized logistic regression
    for 50-50 split.}
    \label{fig:Lambda_pLr}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[clip, trim=4cm 10cm 3cm 10cm, width=\textwidth]{figures/comparison_LR_pLR.pdf}
    \caption{Comparison of logistic regression and penalized logistic regression based on validation accuracy}
    \label{fig:comp_LR_pLR}
  \end{subfigure}
\end{figure}

Best results are obtained on the test accuracy of $97.318\%$ with the values 0.1 for ($\lambda$) and 10.0 for ($\alpha$).
\ref{fig:comp_LR_pLR} shows the comparison of the logistic regression and penalized logistic regression based on the classification accuracy percentage over the test data. The improvements that is captured with the penalized logistic regression is little but significant.  

\begin{table}[h!]
\begin{center}
    \begin{tabular}{ | l | l | l | p{5cm} |}
    \hline
    Method & RMSE & 0-1-Loss & Log-Loss \\ \hline
    Logistic Regression & 0.147896 & 0.020743 & 105.151844 \\ \hline
    Pen Logistic Regression & 0.122288 & 0.014693 & 75.380959 \\ \hline
    \end{tabular}
\end{center}
\caption{Some prediction error estimates for the test data}
\label{table:test_errors}
\end{table}
 
The table \ref{table:test_errors} shows error measurements of the test data with $\alpha=10.0$ and $\lambda=1.0$ using $RMSE$, $0-1 loss$ and $log-loss$. Increasing the value of $\alpha$ decreases all error estimations until a maximum $\lambda=10.0$ value and we obtained the best prediction accuracy with this value for the $\lambda$.