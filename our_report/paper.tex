\documentclass{article} % For LaTeX2e
% We will use NIPS submission format
\usepackage{nips13submit_e,times}
% for hyperlinks
\usepackage{hyperref}
\usepackage{url}
% For figures
\usepackage{graphicx} 
% math packages
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsopn}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}

\title{Project-I by Group Rome}

\author{
Viviana Petrescu\\
EPFL \\
\texttt{vpetresc@epfl.ch} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\nipsfinalcopy 

\begin{document}

\maketitle

\begin{abstract}
We present our on two tasks, classification and regression on data that is not known. We process the data, investigate baseline methods and present our results here.
For the regression task, out best model was <> and for classification was <>.
\end{abstract}

\section{Regression}
\subsection{Data Description}
The training data $Xtrain$ contains 1400 observations, each 43 dimensional. One training sample has 36 real valued features and 7 categorical features. Our task is to predict the values for unseen test data, consisting in 600 samples. We measure the accuracy of our estimation using $RMSE$. 

\subsection{Data visualization and cleaning}
Initially, our data was not centered, as seen in $Fig.$ \ref{fig:dist_regression}.
We changed the categorical features into dummy variables, leading to a new vector of size 56. $Xtrain$ was then normalized to have 0 mean and standard deviation 1. We applied the same operations to the test data $Xtest$ on which we will report the results.

 We plotted the correlation of every feature with respect to the output $ytrain$ and the scatter plots did not look random. We concluded the features explain the output, but we could not tell if one of them is insignificant.

The predicted values were real values in $[1000,7000]$ and seemed to be grouped into two blobs. We believed initially the smaller blob represented outliers, but since it contained almost $10\%$ of the data we decided not to ignore it.

After looking at every feature individually, we noticed that feature 36 offers a clear separation of the two blobs (see $Fig.$ \ref{fig:feature36}). We therefore chose to fit two models, one in which feature 36 has values $>1.4$ (after normalization) and one in which it is smaller, corresponding to smaller predicted values. 


\begin{figure}[h]
  \begin{subfigure}[b]{0.5\textwidth}
   \includegraphics[width=\textwidth]{figures/dist_regression.png}
    \caption{Mean and standard deviation for the first 36 real\newline valued variables of Xtrain. The input is not norma-\newline lised and feature 36 is the only negative one.}
    \label{fig:dist_regression}
  \end{subfigure}
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics[width=\textwidth]{figures/feature36.png}
    \caption{Feature 36 versus output values. X = 1.4 (black line) and y = 4900 (red line)  provide a good separation of the two blobs.}
    \label{fig:feature36}
  \end{subfigure}
  \caption{â€¢ Data visualization. }
\end{figure}

We visually observed some linear correlations between certain features such as feature 2 and 24, 13 and 16, 17 and 20, but we decided to keep them since we did not have time to experiment with their removal or to test their signifcance.
\textcolor{red}{TODO} We also removed some outliers using

\subsection{Baseline methods}
 We experimented with least squares using the normal equations, the least squares using gradient descent with alpha.  In both cases we obtained the same result. TODO talk about ill conditioned matriices.
 We report here our best results and parameters for every model. 
 For finding lambda we used 5 fold cross validation. We can see in figure [] the best results for this.
 The matrix was ill conditioned meaning some of the features were correlated, as expected also from our visualization. As Expected leastSquares gave NaN. We observed ridge regression did not improve much, we used a very small lambda. However, when we used polynomial features of degree 3,  with the same lambda and alpha, we obtained a significant improvement.
 
 Remove outliers for every feature. Plot the features also for the other regression


\subsection{Feature transformations}
We tried to plot


\section{Summary}
In this report, we analyzed a regression dataset and found that ridge regression is a reasonable fit. We estimate that the average test error is 1.213 ($\pm$ 0.02). We tried some feature transformation and found that there is a small improvement giving us a test error of around 1.198 ($\pm$ 0.015). This improvement, however, is not significant.


\subsubsection*{Acknowledgments}
We would like to thank Timur for making the dataset for this study, Carlos and Ilija to help conduct the experiments, and all the students who attended the session making it exciting for us. All the code, as well as this report, was written by Emti. 

\subsubsection*{References}

\end{document}
